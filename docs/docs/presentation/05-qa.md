# 5. Q&A Preparation

**Duration: 2 minutes**

---

## Common Technical Questions

### Architecture & Design

??? question "Why hexagonal architecture for a relatively simple ML project?"

    **Answer**:

    While the current scope is manageable, hexagonal architecture provides:

    1. **Future-proofing**: Easy to add S3, database, or API data sources
    2. **Testability**: Can mock data repository without filesystem dependencies
    3. **Team scalability**: Clear boundaries = less coupling = parallel development
    4. **Learning demonstration**: Shows architectural thinking beyond "just make it work"

    **Trade-off**: Added complexity upfront, but pays off at scale. For a throwaway prototype, YAGNI applies. For a production system, architecture matters.

---

??? question "Why Python Protocols instead of Abstract Base Classes (ABC)?"

    **Answer**:

    **Protocols** (structural subtyping):
    - Duck typing with type safety
    - No inheritance required
    - More Pythonic (closer to Go interfaces)
    - Better for third-party code integration

    **ABCs** (nominal subtyping):
    - Explicit inheritance contract
    - Runtime enforcement
    - Better for internal frameworks

    **Choice**: Protocols for flexibility, especially since I might integrate external libraries that don't inherit from my ABCs.

---

??? question "Why YAML configuration instead of Python code?"

    **Answer**:

    **Benefits**:

    - **Non-engineers** can modify experiments (data scientists, analysts)
    - **Version control** tracks what changed between experiments
    - **Reproducibility** without code execution (just read the YAML)
    - **Hydra** provides powerful composition and overrides

    **Limitations**:

    - Complex logic (e.g., custom transformations) gets verbose
    - YAML parsing errors can be cryptic

    **Hybrid approach**: YAML for parameters, Python functions for complex logic.

---

### ML & Data Science

??? question "How did you decide which features to engineer?"

    **Answer**:

    **Process**:

    1. **Domain research**: Kaggle discussions, real estate knowledge
    2. **EDA (Exploratory Data Analysis)**: Correlation heatmaps, scatter plots
    3. **Hypothesis testing**: "Does quality × area matter more than either alone?"
    4. **Iterative refinement**: Try features, check R² improvement

    **Examples**:

    - **Binary indicators** (HasBasement, HasGarage): Many algorithms don't handle zero well
    - **Polynomial features** (OverallQual²): Non-linear relationships
    - **Interactions** (Quality × Area): High-quality large house ≠ sum of parts

    **Validation**: Feature selection step removes unhelpful features automatically.

---

??? question "Why Ridge over Lasso? Why not XGBoost?"

    **Answer**:

    **Ridge vs. Lasso**:

    - **Ridge** (L2): Shrinks coefficients, keeps all features
    - **Lasso** (L1): Can zero out coefficients (feature selection)
    - **Result**: Ridge performed 2% better (R² 0.8923 vs. 0.8734)

    **Why not tree-based models (XGBoost, Random Forest)?**

    - **Focus**: Demonstrating engineering practices, not leaderboard optimization
    - **Interpretability**: Linear models easier to explain to stakeholders
    - **Future work**: Would add gradient boosting for production

    **Trade-off**: Simpler model, slightly lower accuracy, but better demonstration of feature engineering.

---

??? question "How do you handle missing data?"

    **Answer**:

    **Strategy varies by missingness**:

    1. **>90% missing** (PoolQC, Alley): **Drop** - not enough information
    2. **Numerical**: **Median** imputation (robust to outliers)
    3. **Categorical**: **Mode** (most common value)

    **Why not fancier methods** (KNN, iterative imputation)?

    - **Complexity**: Adds computation time and tuning
    - **Marginal benefit**: Median/mode work well for this dataset
    - **Interpretability**: Simple methods easier to explain

    **Production consideration**: Would use multiple imputation for critical features.

---

### Testing & Quality

??? question "How do you test ML code? It's non-deterministic!"

    **Answer**:

    **Testing strategy**:

    1. **Unit tests**: Test transformers with fixed `random_state`
       ```python
       def test_imputation():
           data = pd.DataFrame({"A": [1, np.nan, 3]})
           result = ImputationTransformer().fit_transform(data)
           assert result["A"].iloc[1] == 2.0  # Median
       ```

    2. **Integration tests**: Full pipeline with fixed seed
       ```python
       def test_experiment_pipeline():
           result = run_experiment(config, random_state=42)
           assert 0.85 < result.r2_score < 0.95  # Range check
       ```

    3. **Property-based tests**: Check invariants
       ```python
       def test_scaling_properties():
           scaled = scaler.fit_transform(data)
           assert abs(scaled.mean()) < 1e-10  # Mean ≈ 0
           assert abs(scaled.std() - 1) < 1e-10  # Std ≈ 1
       ```

    **Key**: Test behaviors and properties, not exact values.

---

??? question "How do you ensure experiments are reproducible?"

    **Answer**:

    **Multi-layered approach**:

    1. **Fixed seeds**: `random_state=37` in config
    2. **Dependency pinning**: `uv.lock` locks exact versions
    3. **MLflow tracking**: Logs config, code version, environment
    4. **YAML configs**: Version controlled experiment parameters
    5. **Docker** (future): Containerize entire environment

    **Verification**:

    ```bash
    # Run same config twice
    uv run -m src.cli experiment --config-name exp1
    uv run -m src.cli experiment --config-name exp1
    # Check MLflow: metrics should be identical
    ```

---

### Tools & Workflow

??? question "Why uv instead of pip/poetry?"

    **Answer**:

    **Performance**:

    - **uv**: 10-100x faster than pip (written in Rust)
    - **poetry**: Slow resolver, large projects timeout

    **Features**:

    - Lock file (reproducibility)
    - Virtual env management
    - Caching (repeated installs are instant)

    **Trade-off**: Newer tool, smaller community, but backed by Astral (also builds ruff).

---

??? question "How does MLflow compare to Weights & Biases or Neptune?"

    **Answer**:

    | Feature | MLflow | W&B | Neptune |
    |---------|--------|-----|---------|
    | **Hosting** | Self-hosted | Cloud | Cloud |
    | **Cost** | Free | Free tier + paid | Paid |
    | **UI** | Simple | Rich | Rich |
    | **Integrations** | sklearn, TF, PyTorch | Everything | Everything |

    **Why MLflow**:

    - **Self-hosted**: No data leaves my machine
    - **Open source**: No vendor lock-in
    - **Sufficient**: Tracks what I need (params, metrics, artifacts)

    **When to use W&B/Neptune**:

    - Team collaboration (shared cloud experiments)
    - Rich visualizations (learning curves, confusion matrices)
    - Sweep/hyperparameter tuning built-in

---

## Scenario Questions

??? question "How would you deploy this to production?"

    **Answer**:

    **Deployment architecture**:

    ```
    User Request
        ↓
    FastAPI REST Endpoint
        ↓
    Load Model from MLflow Registry
        ↓
    Preprocess Input (same pipeline)
        ↓
    Predict Price
        ↓
    Return JSON Response
    ```

    **Steps**:

    1. **Containerize**: Dockerfile with Python 3.12, dependencies
    2. **Model Registry**: Register best model in MLflow
    3. **API**: FastAPI endpoint (`POST /predict`)
    4. **Monitoring**: Log predictions, latency, errors
    5. **CI/CD**: GitHub Actions (test → build → deploy)

    **Infrastructure**:

    - **Dev/Staging**: Docker Compose
    - **Production**: Kubernetes (GKE/EKS) or Cloud Run
    - **Monitoring**: Prometheus + Grafana, or Datadog

---

??? question "How would you handle model drift in production?"

    **Answer**:

    **Detection**:

    1. **Input drift**: Monitor feature distributions
       - Alert if mean/std shifts >2σ from training data
    2. **Output drift**: Track prediction distributions
       - Alert if price distribution changes significantly
    3. **Performance drift**: Track actual vs. predicted (if labels available)
       - Retrain if MAE increases >10%

    **Tools**:

    - **Evidently AI**: Open source drift detection
    - **Whylabs**: Feature monitoring
    - **Custom**: Log features/predictions to data warehouse, run scheduled checks

    **Response**:

    1. **Investigate**: Is it real change or data issue?
    2. **Retrain**: On recent data if drift confirmed
    3. **A/B test**: New model vs. old before full rollout

---

??? question "How would you scale this to millions of predictions/day?"

    **Answer**:

    **Current bottlenecks**:

    1. **Model loading**: Loads from disk on each request
    2. **Preprocessing**: Python loops, not vectorized
    3. **Single instance**: No horizontal scaling

    **Solutions**:

    1. **Model caching**: Load once, keep in memory
    2. **Batch predictions**: Process 1000s at once, not one-by-one
    3. **Async processing**: Queue (Redis/RabbitMQ) + workers
    4. **Horizontal scaling**: Multiple replicas behind load balancer
    5. **Optimization**: ONNX runtime (10x faster inference), quantization

    **Architecture**:

    ```
    API Gateway (rate limiting)
        ↓
    Load Balancer
        ↓
    Multiple Prediction Services (cached models)
        ↓
    Model artifacts from S3/GCS
    ```

---

## Behavioral Questions

??? question "Describe a technical challenge you faced in this project."

    **Answer**:

    **Challenge**: Feature engineering YAML became unwieldy for complex interactions.

    **Situation**: Wanted to create interaction features (Quality × Area, Age × Condition), but YAML syntax got verbose and hard to read.

    **Action**:

    1. Evaluated options: Python functions, DSL, keep YAML
    2. Decided on hybrid: YAML for simple features, Python decorators for complex
    3. Created `@feature_engineer` decorator that registers functions
    4. Updated YAML to reference function names

    **Result**: Cleaner YAML, more flexible feature engineering, easier to unit test.

    **Learning**: YAML is great for configuration, but code is better for logic.

---

??? question "How do you prioritize what to work on?"

    **Answer**:

    **Framework**: Impact × Effort matrix

    | | High Impact | Low Impact |
    |---|---|---|
    | **Low Effort** | ✅ **Do first** (outlier removal) | 🟡 Quick wins |
    | **High Effort** | 🟠 Plan carefully (hexagonal arch) | ❌ Skip (custom viz lib) |

    **Example**:

    - **High impact, low effort**: Feature engineering → immediate R² boost
    - **High impact, high effort**: Hexagonal architecture → long-term maintainability
    - **Low impact, high effort**: Custom visualization library → use Plotly instead

    **Communication**: Align with stakeholders on what "impact" means (accuracy vs. speed vs. interpretability).

---

## Red Flags to Avoid

!!! danger "Don't Say"
    - "I just followed a tutorial" - Shows lack of depth
    - "I don't know" without context - Better: "I haven't worked with X, but I'd approach it by..."
    - "This is the best architecture" - Everything has trade-offs
    - "Tests are optional" - Red flag for production code

!!! success "Do Say"
    - "I chose X over Y because..." - Shows intentional decision-making
    - "I'd improve this by..." - Demonstrates growth mindset
    - "The trade-off is..." - Shows nuanced thinking
    - "I learned that..." - Shows reflection and learning

---

## Closing Statement

If asked **"Any final thoughts?"**:

> "This project demonstrates how I approach ML problems: not just building models, but building **systems**. I focused on architecture, testing, and reproducibility because I believe the best ML model is one that can be understood, maintained, and deployed by a team. I'm excited to bring this engineering mindset to [Company Name] and learn from your team's best practices."

---

[Back to Home](../index.md) | [Review Presentation Flow](01-problem.md)
